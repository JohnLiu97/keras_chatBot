{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from operator import itemgetter\n",
    "from copy import deepcopy\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "\n",
    "    # lower the words\n",
    "    text = text.lower()\n",
    "    # Extend some abbreviations to original form\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    \n",
    "    # Remove all the symbols \n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+*=~|.!?,]\", \"\", text)\n",
    "    \n",
    "    # Convert more than one space between two words to one space\n",
    "    text = re.sub(' +',' ',text)\n",
    "    return text\n",
    "\n",
    "def line_sort(movie):\n",
    "    new_frame = deepcopy(movie)\n",
    "    for ind, i in enumerate(movie):\n",
    "        line_seq = int(re.search('L(.*)', i[0]).group(1))\n",
    "        \n",
    "        # strip the sentence\n",
    "        new_frame[ind][4] = new_frame[ind][4].strip()\n",
    "        new_frame[ind][0] = line_seq\n",
    "\n",
    "    #  Sort each sentence based on line ID in ascend order\n",
    "\n",
    "    new_frame = sorted(new_frame, key=itemgetter(0))\n",
    "    return new_frame\n",
    "\n",
    "# Slice many stories based on the continuous line ID. \n",
    "def scenario_generator(a_movie):\n",
    "    story = []\n",
    "    people = set()\n",
    "    \n",
    "    sorted_a_movie = line_sort(a_movie)\n",
    "    a_sentence = []\n",
    "    for ind, am in enumerate(sorted_a_movie):\n",
    "        if (ind + 1 < len(sorted_a_movie)) and (sorted_a_movie[ind+1][0]) == (am[0] + 1):\n",
    "            people.add(am[1])\n",
    "            \n",
    "# we assume that in a story, there are two characters having conversation. \n",
    "            if len(people) < 3:\n",
    "                a_sentence.append(am[4]) \n",
    "            else:\n",
    "                story.append(a_sentence)\n",
    "                a_sentence = []\n",
    "                a_sentence.append(am[4])\n",
    "                people = set()\n",
    "            \n",
    "        else:\n",
    "            a_sentence.append(am[4])\n",
    "            \n",
    "            story.append(a_sentence)\n",
    "            a_sentence = []\n",
    "            people = set()\n",
    "\n",
    "    return story\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dialogue = []\n",
    "error_less = []\n",
    "error_greater = []\n",
    "\n",
    "with open('movie_lines.tsv') as f:\n",
    "    lines = f.read().split('\\n')[:-1]\n",
    "    \n",
    "dialogue = []\n",
    "for row in lines:\n",
    "    row_li = row.split('\\t')\n",
    "    modify_row = ' '.join(row_li[4:])\n",
    "    modify_row = re.sub(' +',' ',modify_row)\n",
    "    modify_row = modify_row.replace(\" \\' \",\"'\")\n",
    "    if len(row_li) > 5:\n",
    "\n",
    "        dialogue.append(row_li[:4] + [modify_row])\n",
    "    else:\n",
    "        dialogue.append(row_li[:4] + [modify_row])\n",
    "        \n",
    "movie_dict = dict()\n",
    "\n",
    "for i in dialogue:\n",
    "    if i[2] not in movie_dict:\n",
    "        movie_dict[i[2]] = []\n",
    "    movie_dict[i[2]].append(i)\n",
    "    \n",
    "convs = {}\n",
    "\n",
    "for k, v in movie_dict.items():\n",
    "    convs[k] = scenario_generator(v)\n",
    "    \n",
    "# Sort the sentences into questions (inputs) and answers (targets)\n",
    "questions = []\n",
    "answers = []\n",
    "for s in convs.values():\n",
    "    for conv in list(s):\n",
    "        for i in range(len(conv)-1):\n",
    "            questions.append(conv[i])\n",
    "            answers.append(conv[i+1])\n",
    "            \n",
    "\n",
    "# Clean the data\n",
    "clean_questions = []\n",
    "for question in questions:\n",
    "    clean_questions.append(clean_text(question))\n",
    "clean_questions =  clean_questions \n",
    "clean_answers = []    \n",
    "for answer in answers:\n",
    "    clean_answers.append(clean_text(answer))\n",
    "clean_answers =  clean_answers   \n",
    "\n",
    "# Find the length of sentences\n",
    "lengths = []\n",
    "for question in clean_questions:\n",
    "    lengths.append(len(question.split()))\n",
    "for answer in clean_answers:\n",
    "    lengths.append(len(answer.split()))\n",
    "\n",
    "# Create a dataframe so that the values can be inspected\n",
    "lengths = pd.DataFrame(lengths, columns=['counts'])\n",
    "\n",
    "# Remove questions and answers that are shorter than 2 words and longer than 20 words.\n",
    "min_line_length = 2\n",
    "max_line_length = 20\n",
    "\n",
    "# Filter out the questions that are too short/long\n",
    "short_questions_temp = []\n",
    "short_answers_temp = []\n",
    "\n",
    "i = 0\n",
    "for question in clean_questions:\n",
    "    if len(question.split()) >= min_line_length and len(question.split()) <= max_line_length:\n",
    "        short_questions_temp.append(question)\n",
    "        short_answers_temp.append(clean_answers[i])\n",
    "    i += 1\n",
    "\n",
    "# Filter out the answers that are too short/long\n",
    "short_questions = []\n",
    "short_answers = []\n",
    "\n",
    "i = 0\n",
    "for answer in short_answers_temp:\n",
    "    if len(answer.split()) >= min_line_length and len(answer.split()) <= max_line_length:\n",
    "        short_answers.append(answer)\n",
    "        short_questions.append(short_questions_temp[i])\n",
    "    i += 1\n",
    "    \n",
    "# Create a dictionary for the frequency of the vocabulary\n",
    "vocab = {}\n",
    "for question in short_questions:\n",
    "    for word in question.split():\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1\n",
    "            \n",
    "for answer in short_answers:\n",
    "    for word in answer.split():\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1\n",
    "            \n",
    "# Remove rare words from the vocabulary.\n",
    "# We will aim to replace fewer than 5% of words with <UNK>\n",
    "# You will see this ratio soon.\n",
    "threshold = 10\n",
    "count = 0\n",
    "for k,v in vocab.items():\n",
    "    if v >= threshold:\n",
    "        count += 1\n",
    "        \n",
    "# In case we want to use a different vocabulary sizes for the source and target text, \n",
    "# we can set different threshold values.\n",
    "# Nonetheless, we will create dictionaries to provide a unique integer for each word.\n",
    "questions_vocab_to_int = {}\n",
    "\n",
    "word_num = 0\n",
    "for word, count in vocab.items():\n",
    "    if count >= threshold:\n",
    "        questions_vocab_to_int[word] = word_num\n",
    "        word_num += 1\n",
    "        \n",
    "answers_vocab_to_int = {}\n",
    "\n",
    "word_num = 0\n",
    "for word, count in vocab.items():\n",
    "    if count >= threshold:\n",
    "        answers_vocab_to_int[word] = word_num\n",
    "        word_num += 1\n",
    "        \n",
    "# Add <EOS>, <Start>, and <UNK> to vocab and modify the sentences by them\n",
    "\n",
    "codes = ['<EOS>','<UNK>','<Start>']\n",
    "\n",
    "for code in codes:\n",
    "    questions_vocab_to_int[code] = len(questions_vocab_to_int)+1\n",
    "    \n",
    "for code in codes:\n",
    "    answers_vocab_to_int[code] = len(answers_vocab_to_int)+1\n",
    "    \n",
    "# Create dictionaries to map the unique integers to their respective words.\n",
    "# i.e. an inverse dictionary for vocab_to_int.\n",
    "questions_int_to_vocab = {v_i: v for v, v_i in questions_vocab_to_int.items()}\n",
    "answers_int_to_vocab = {v_i: v for v, v_i in answers_vocab_to_int.items()}\n",
    "\n",
    "modified_short_questions = []\n",
    "\n",
    "for i in range(len(short_answers)):\n",
    "    short_answers[i] += ' <EOS>'\n",
    "    \n",
    "# Add the end of sentence token to the end of every answer.\n",
    "for i in range(len(short_questions)):\n",
    "    short_questions[i] += ' <EOS>'\n",
    "    \n",
    "# Convert the text to integers. \n",
    "# Replace any words that are not in the respective vocabulary with <UNK> \n",
    "questions_int = []\n",
    "for question in short_questions:\n",
    "    ints = []\n",
    "    for word in question.split():\n",
    "        if word not in questions_vocab_to_int:\n",
    "            ints.append(questions_vocab_to_int['<UNK>'])\n",
    "        else:\n",
    "            ints.append(questions_vocab_to_int[word])\n",
    "    questions_int.append(ints)\n",
    "    \n",
    "answers_int = []\n",
    "for answer in short_answers:\n",
    "    ints = []\n",
    "    for word in answer.split():\n",
    "        if word not in answers_vocab_to_int:\n",
    "            ints.append(answers_vocab_to_int['<UNK>'])\n",
    "        else:\n",
    "            ints.append(answers_vocab_to_int[word])\n",
    "    answers_int.append(ints)\n",
    "    \n",
    "# Sort questions and answers by the length of questions.\n",
    "# This will reduce the amount of padding during training\n",
    "# Which should speed up training and help to reduce the loss\n",
    "\n",
    "sorted_questions = []\n",
    "sorted_answers = []\n",
    "\n",
    "for length in range(1, max_line_length+1):\n",
    "    for i in enumerate(questions_int):\n",
    "        if len(i[1]) == length:\n",
    "            sorted_questions.append(questions_int[i[0]])\n",
    "            sorted_answers.append(answers_int[i[0]])\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000\n",
      "Number of unique input tokens: 8739\n",
      "Number of unique output tokens: 8739\n",
      "Max sequence length for inputs: 21\n",
      "Max sequence length for outputs: 21\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 8739)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None, 8739)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 128), (None, 4540416     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 128),  4540416     input_2[0][0]                    \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 8739)   1127331     lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 10,208,163\n",
      "Trainable params: 10,208,163\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128  # Batch size for training.\n",
    "epochs = 5  # Number of epochs to train for.\n",
    "latent_dim = 128  # Latent dimensionality of the encoding space.\n",
    "num_samples = 10000  # Number of samples to train on.\n",
    "# Path to the data txt file on disk.\n",
    "\n",
    "# Vectorize the data.\n",
    "input_texts = short_questions\n",
    "target_texts = short_answers\n",
    "\n",
    "input_characters = sorted(list(set(questions_int_to_vocab.values())))\n",
    "target_characters = sorted(list(set(answers_int_to_vocab.values())))\n",
    "\n",
    "\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt.split()) for txt in input_texts[:num_samples]])\n",
    "max_decoder_seq_length = max([len(txt.split()) for txt in target_texts[:num_samples]])\n",
    "\n",
    "print('Number of samples:', len(input_texts[:num_samples]))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "\n",
    "input_token_index = questions_vocab_to_int\n",
    "target_token_index = answers_vocab_to_int\n",
    "\n",
    "\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts[:num_samples]), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype= np.float32)\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts[:num_samples]), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype= np.float32)\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts[:num_samples]), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype= np.float32)\n",
    "\n",
    "\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts[:num_samples], target_texts[:num_samples])):\n",
    "    for t, char in enumerate(input_text.split()):\n",
    "        try:\n",
    "            encoder_input_data[i, t, input_token_index[char.lower()]] = 1.\n",
    "        except KeyError:\n",
    "            encoder_input_data[i, t, input_token_index['<UNK>']] = 1.\n",
    "\n",
    "    for t, char in enumerate(target_text.split()):\n",
    "#         print(t)\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        try:\n",
    "            decoder_input_data[i, t, target_token_index[char.lower()]] = 1.\n",
    "        except KeyError:\n",
    "            decoder_input_data[i, t, target_token_index['<UNK>']] = 1.\n",
    "        \n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            try:\n",
    "                decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "            except KeyError:\n",
    "                decoder_target_data[i, t - 1, target_token_index['<UNK>']] = 1.\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the \n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "print(model.summary())\n",
    "\n",
    "# model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "# model.fit(train_questions, train_answers, batch_size=128, epochs=10)\n",
    "model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.utils import plot_model\n",
    "# plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model from .h5 checkpoint\n",
    "#### Fit the model(if loaded, you don't have to run the fitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_ = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "if load_ == True:\n",
    "    model = load_model('chatModel.h5')\n",
    "    \n",
    "else:\n",
    "    model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=1000,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    decoder_length = 25\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    #target_seq[0, 0, target_token_index['<Start>']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "        \n",
    "        #print(output_tokens)\n",
    "        \n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "        decoded_sentence += ' '\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '<EOS>' or\n",
    "           len(decoded_sentence) > decoder_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run chating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================\n",
      "Human:Hi\n",
      "Chatbot: dumping carlo v'ger merrick \n",
      "==========================\n",
      "Human:How are you\n",
      "Chatbot: hair fits fee compound doug \n",
      "==========================\n",
      "Human:What's my name\n",
      "Chatbot: control puppies lady's union \n",
      "==========================\n",
      "Human:whta's your name\n",
      "Chatbot: channing ripped conner umm \n",
      "==========================\n"
     ]
    }
   ],
   "source": [
    "def get_input_embedding(string):\n",
    "    wordNum = 0\n",
    "    stringVec = np.zeros([1,21,num_encoder_tokens])\n",
    "    for word in string.split():\n",
    "        if not word in ['<UNK>','<EOS>']:\n",
    "            spl_word = clean_text(word).split()\n",
    "            for word in spl_word:\n",
    "                try:\n",
    "                    idx = questions_vocab_to_int[word.lower()]\n",
    "                    wordVec = np.zeros(num_encoder_tokens)\n",
    "                    wordVec[idx] = 1\n",
    "                    stringVec[0,wordNum] = wordVec\n",
    "                except KeyError:\n",
    "                    continue\n",
    "            wordNum += 1\n",
    "        if wordNum>=21:\n",
    "            break\n",
    "    \n",
    "    return  stringVec\n",
    "\n",
    "while True:\n",
    "    print('==========================')\n",
    "    input_seq = input(\"Human:\")\n",
    "\n",
    "    decoded_sentence = decode_sequence(get_input_embedding(input_seq))\n",
    "\n",
    "    print('Chatbot:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
